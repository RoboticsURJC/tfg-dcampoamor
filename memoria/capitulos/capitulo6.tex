\chapter{Conclusiones}
\label{cap:capitulo6}

En este último capítulo se exponen las conclusiones generales del trabajo realizado, detallando el grado de cumplimiento de los objetivos y requisitos planteados al inicio del proyecto. Asimismo, %se reflejan las competencias técnicas y personales adquiridas durante su desarrollo. Por último, 
se presentan algunas posibles líneas de mejora y evolución que podrían ser exploradas en futuros trabajos, con el fin de dar continuidad y ampliar el alcance del sistema propuesto.

\section{Objetivos y requisitos cumplidos}
\label{sec:objetivos_y_requisitos}

A continuación, se van a explicar todos los objetivos y requisitos cumplidos en la realización del presente trabajo fin de grado.

\subsection{Objetivos}
\label{subsec:objetivos}

Se ha conseguido cumplir con el objetivo principal de este Trabajo Fin de Grado: desarrollar un sistema de visión artificial de bajo coste, basado en técnicas de inteligencia artificial, capaz de detectar fresas maduras y comunicar su posición y distancia a un brazo robótico para su recolección automatizada. Todo ello ha sido probado tanto en entornos simulados como en condiciones reales, demostrando la viabilidad del sistema diseñado.

A su vez, se han cumplido todos los objetivos definidos en la Sección \ref{sec:descripcion}:

\begin{enumerate}
  \item Se han investigado las soluciones actuales relacionadas con la detección de frutos mediante visión artificial, encontrándose una gran variedad de propuestas tanto académicas como comerciales, muchas de ellas aún en desarrollo o centradas en otros tipos de cultivos.
  \item Se ha seleccionado la técnica de inteligencia artificial más adecuada para el reconocimiento de fresas, optando por el uso de redes neuronales convolucionales (CNN) a través del modelo YOLOv3, debido a su eficiencia y precisión en tareas de detección en tiempo real. Asimismo, se han elegido los componentes hardware necesarios para implementar un sistema de visión robusto y de bajo coste.
  \item La técnica escogida se ha optimizado y adaptado para funcionar en la plataforma de trabajo, lo que ha requerido la creación de un dataset específico de imágenes de fresas. Este conjunto de datos fue tratado adecuadamente para garantizar la calidad del entrenamiento y mejorar la precisión del modelo final.
  \item Se ha realizado el entrenamiento del sistema con distintos algoritmos de clasificación basados en Machine Learning, evaluando su rendimiento mediante pruebas con imágenes reales. El modelo YOLOv3 ha ofrecido el mejor equilibrio entre velocidad y precisión.
  \item Se ha seleccionado el protocolo de comunicación entre el sistema de visión y el robot, implementando un servidor XML-RPC para transmitir de forma efectiva la información de las detecciones. Este protocolo ha sido validado mediante pruebas tanto en simulador como en el entorno real del robot.
  \item Se ha dotado al sistema de software capaz de reconocer fresas maduras, calcular su posición en coordenadas del mundo real y estimar su distancia a la cámara, información que se guarda y se transmite al brazo robótico para su uso operativo.
  \item Finalmente, se ha probado el sistema completo en situaciones tanto simuladas como reales, comprobando su funcionamiento y eficiencia, y sentando las bases para posibles mejoras y aplicaciones futuras.
\end{enumerate}

\subsection{Requisitos}
\label{subsec:requisitos}

También cabe destacar que se han satisfecho todos los requisitos planteados en la Sección \ref{sec:requisitos}:

\begin{enumerate}
    \item Se ha utilizado como sistema operativo la distribución Ubuntu 22.04 LTS sobre GNU/Linux, cumpliendo así con el requisito de utilizar software libre y con soporte a largo plazo para la ejecución del programa del sistema de visión.
    \item Los modelos entrenados han sido optimizados %y convertidos al formato adecuado 
    para ajustarse a las limitaciones del hardware utilizado, asegurando su correcto funcionamiento sin necesidad de recursos computacionales de alto rendimiento.
    \item El sistema ha demostrado ser capaz de operar en tiempo real, realizando la detección de fresas, el cálculo de distancias y la transmisión de datos al robot con una latencia reducida, adecuada para su uso práctico.
    \item Todo el hardware empleado en el desarrollo del sistema de visión ha sido seleccionado con un criterio de bajo coste, haciendo que %el sistema completo 
    sea accesible para estudiantes o centros con recursos limitados.
    \item La aplicación final es fácilmente reproducible y desplegable tanto en un entorno simulado %mediante el uso del simulador oficial del fabricante del robot, 
    como en condiciones reales, permitiendo su integración en contextos educativos o de laboratorio sin dificultades técnicas significativas.
\end{enumerate}

\section{Líneas Futuras}
\label{sec:lineas_futuras}

A partir de los resultados obtenidos en este proyecto, se identifican diversas líneas de trabajo que podrían abordarse en el futuro para mejorar y ampliar el sistema desarrollado:

\begin{itemize}
    \item Entrenamiento con datasets más amplios y variados: Ampliar el conjunto de imágenes utilizado para entrenar el modelo de detección, incluyendo diferentes %condiciones de iluminación, ángulos de visión y 
    niveles de maduración, con el fin de mejorar la robustez y generalización del sistema.
    \item Optimización del rendimiento en hardware embebido: Adaptar el sistema para funcionar en dispositivos aún más limitados (como Raspberry Pi Zero o NVIDIA Jetson Nano), buscando reducir el consumo energético y el coste del sistema.
    \item Implementación en entornos agrícolas reales: Validar el sistema en condiciones reales de campo, frente a variables como viento, sombra o vegetación densa para comprobar su fiabilidad y utilidad práctica.
    \item Diseño o integración de una herramienta de recolección versátil: Investigar o desarrollar una pinza robótica compatible con el sistema de visión y adecuada para manipular distintos tipos de frutos con cuidado y precisión, ampliando así el alcance del sistema a otros cultivos más allá de la fresa. 
\end{itemize}
